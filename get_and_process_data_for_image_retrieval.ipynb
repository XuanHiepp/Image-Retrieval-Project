{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Crawl URL from Website"
      ],
      "metadata": {
        "id": "HpaKUYRa65gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y wget\n",
        "!pip install selenium\n",
        "!apt-get install -y chromium-browser\n",
        "!apt-get install -y chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "from bs4 import BeautifulSoup # For parsing HTML content\n",
        "from urllib.parse import urljoin, urlparse # For handling URLs\n",
        "import urllib.request # For making HTTP requests\n",
        "import time # For handling time-related operations\n",
        "import os # For interacting with the operating system (relate to dir, folder, file)\n",
        "from tqdm import tqdm # For displaying progress bars (visualize progress)\n",
        "import concurrent.futures # For multi-threading\n",
        "import json # For writing to a text file\n",
        "from PIL import Image # For handling images"
      ],
      "metadata": {
        "id": "ueZKmOAW6630"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UrlScraper:\n",
        "    # Constructor\n",
        "    def __init__(self, url_template, max_images=50, max_workers=4):\n",
        "        self.url_template = url_template # Link crawl\n",
        "        self.max_images = max_images # Max images\n",
        "        self.max_workers = max_workers # Thread\n",
        "        self.setup_environment() # Call for set up environment\n",
        "\n",
        "    # Set up environment for selenium\n",
        "    def setup_environment(self):\n",
        "        os.environ[\"PATH\"] += \":/usr/lib/chromium-browser/\"\n",
        "        os.environ[\"PATH\"] += \":/usr/lib/chromium-browser/chromedriver/\"\n",
        "\n",
        "    def get_url_images(self, term):\n",
        "        \"\"\"\n",
        "        Crawl the urls of images by term\n",
        "\n",
        "        Parameters:\n",
        "        term (str): The name of animal, plant, scenery, furniture\n",
        "\n",
        "        Returns:\n",
        "        urls (list): List of urls of images\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize Chrome driver\n",
        "        options = webdriver.ChromeOptions()\n",
        "        options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        driver = webdriver.Chrome(options=options)\n",
        "\n",
        "        url = self.url_template.format(search_term=term)\n",
        "        driver.get(url)\n",
        "\n",
        "        # Start crawl urls of image like brute force - the same mechanism with this but add some feature\n",
        "        urls = []\n",
        "        more_content_available = True\n",
        "\n",
        "        pbar = tqdm(total=self.max_images, desc=f\"Fetching images for {term}\") # Set up for visualize progress\n",
        "\n",
        "        while len(urls) < self.max_images and more_content_available:\n",
        "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "            img_tags = soup.find_all(\"img\")\n",
        "\n",
        "            for img in img_tags:\n",
        "                if len(urls) >= self.max_images:\n",
        "                    break\n",
        "                if \"src\" in img.attrs:\n",
        "                    href = img.attrs[\"src\"]\n",
        "                    img_path = urljoin(url, href)\n",
        "                    img_path = img_path.replace(\"_m.jpg\", \"_b.jpg\").replace(\"_n.jpg\", \"_b.jpg\").replace(\"_w.jpg\", \"_b.jpg\")\n",
        "                    if img_path == \"https://combo.staticflickr.com/ap/build/images/getty/IStock_corporate_logo.svg\":\n",
        "                        continue\n",
        "                    urls.append(img_path)\n",
        "                    pbar.update(1)\n",
        "\n",
        "            try:\n",
        "                load_more_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='yui_3_16_0_1_1721642285931_28620']\")))\n",
        "                load_more_button.click()\n",
        "                time.sleep(2)\n",
        "            except:\n",
        "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "                time.sleep(2)\n",
        "\n",
        "                new_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "                new_img_tags = new_soup.find_all(\"img\", loading_=\"lazy\")\n",
        "                if len(new_img_tags) == len(img_tags):\n",
        "                    more_content_available = False\n",
        "                img_tags = new_img_tags\n",
        "\n",
        "        pbar.close()\n",
        "        driver.quit()\n",
        "        return urls\n",
        "\n",
        "    def scrape_urls(self, categories):\n",
        "        \"\"\"\n",
        "        Call get_url_images method to get all urls of any object in categories\n",
        "\n",
        "        Parameter:\n",
        "        categories (dictionary): the dict of all object we need to collect image with format\n",
        "            categories{\"name_object\": [value1, value2, ...]}\n",
        "\n",
        "        Returns:\n",
        "        all_urls (dictionary): Dictionary of urls of images\n",
        "        \"\"\"\n",
        "        all_urls = {category: {} for category in categories}\n",
        "\n",
        "        # Handle multi-threading for efficent installation\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            future_to_term = {executor.submit(self.get_url_images, term): (category, term) for category, terms in categories.items() for term in terms}\n",
        "\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_term), total=len(\n",
        "                future_to_term), desc=\"Overall Progress\"):\n",
        "                category, term = future_to_term[future]\n",
        "                try:\n",
        "                    urls = future.result()\n",
        "                    all_urls[category][term] = urls\n",
        "                    print(f\"\\nNumber of images retrieved for {term}: {len(urls)}\")\n",
        "                except Exception as exc:\n",
        "                    print(f\"\\n{term} generated an exception: {exc}\")\n",
        "        return all_urls\n",
        "\n",
        "    def save_to_file(self, data, filename):\n",
        "        \"\"\"\n",
        "        Save the data to a JSON file.\n",
        "\n",
        "        Parameters:\n",
        "        data (dict): The data to be saved.\n",
        "        filename (str): The name of the JSON file.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        with open(filename, \"w\") as file:\n",
        "            json.dump(data, file, indent=4)\n",
        "        print(f\"Data saved to {filename}\")"
      ],
      "metadata": {
        "id": "fBhNV7C-7N-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = {\"animal\": [\"Monkey\", \"Elephant\", \"cows\", \"Cat\", \"Dog\", \"bear\", \"fox\", \"Civet\", \"Pangolins\",\n",
        "                         \"Rabbit\", \"Bats\", \"Whale\", \"Cock\", \"Owl\", \"flamingo\", \"Lizard\", \"Turtle\", \"Snake\",\n",
        "                         \"Frog\", \"Fish\", \"shrimp\", \"Crab\", \"Snail\", \"Coral\", \"Jellyfish\", \"Butterfly\", \"Flies\",\n",
        "                         \"Mosquito\", \"Ants\", \"Cockroaches\", \"Spider\", \"scorpion\", \"tiger\", \"bird\", \"horse\",\n",
        "                         \"pig\", \"Alligator\", \"Alpaca\", \"Anteater\", \"donkey\", \"Bee\", \"Buffalo\", \"Camel\",\n",
        "                         \"Caterpillar\", \"Cheetah\", \"Chicken\", \"Dragonfly\", \"Duck\", \"panda\", \"Giraffe\"],\n",
        "              \"plant\": [\"Bamboo\", \"Apple\", \"Apricot\", \"Banana\", \"Bean\", \"Wildflower\", \"Flower\",\n",
        "                        \"Mushroom\", \"Weed\", \"Fern\", \"Reed\", \"Shrub\", \"Moss\", \"Grass\", \"Palmtree\", \"Corn\",\n",
        "                        \"Tulip\", \"Rose\", \"Clove\", \"Dogwood\", \"Durian\", \"Ferns\", \"Fig\", \"Flax\", \"Frangipani\",\n",
        "                        \"Lantana\", \"Hibiscus\", \"Bougainvillea\", \"Pea\", \"OrchidTree\", \"RangoonCreeper\",\n",
        "                        \"Jackfruit\", \"Cottonplant\", \"Corneliantree\", \"Coffeeplant\", \"Coconut\", \"wheat\",\n",
        "                        \"watermelon\", \"radish\", \"carrot\"],\n",
        "              \"furniture\": [\"bed\", \"cabinet\", \"chair\", \"chests\", \"clock\", \"desks\", \"table\", \"Piano\",\n",
        "                            \"Bookcase\", \"Umbrella\", \"Clothes\", \"cart\", \"sofa\", \"ball\", \"spoon\", \"Bowl\", \"fridge\",\n",
        "                            \"pan\", \"book\"],\n",
        "              \"scenery\": [\"Cliff\", \"Bay\", \"Coast\", \"Mountains\", \"Forests\", \"Waterbodies\", \"Lake\",\n",
        "                          \"desert\", \"farmland\", \"river\", \"hedges\", \"plain\", \"sky\", \"cave\", \"cloud\", \"flowergarden\",\n",
        "                          \"glacier\", \"grassland\", \"horizon\", \"lighthouse\", \"plateau\", \"savannah\", \"valley\", \"volcano\", \"waterfall\"]}\n",
        "\n",
        "urltopic = {\"flickr\": \"https://www.flickr.com/search/?text={search_term}\"}\n",
        "scraper = UrlScraper(url_template=urltopic[\"flickr\"], max_images=20, max_workers=5)\n",
        "image_urls = scraper.scrape_urls(categories)\n",
        "scraper.save_to_file(image_urls, \"image_urls.json\")"
      ],
      "metadata": {
        "id": "05_KMrIJ7ePq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Get the data"
      ],
      "metadata": {
        "id": "EMf1ZLoIAn2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDownloader:\n",
        "    def __init__(self, json_file, download_dir=\"Dataset\", max_workers=4, delay=1):\n",
        "        self.json_file = json_file # file containing URLs of images in JSON format\n",
        "        self.download_dir = download_dir # Folder name for storing images\n",
        "        self.max_workers = max_workers # Number of threads\n",
        "        self.delay = delay # Polite delay: when we send request too much to the server for downloading images without polite delay, it will crash or prevent your IP\n",
        "        self.filename = set() # To store filename directories\n",
        "        self.setup_directory() # Set up the folder structure\n",
        "\n",
        "        def setup_directory(self):\n",
        "        if not os.path.exists(self.download_dir):\n",
        "        os.makedirs(self.download_dir)\n",
        "\n",
        "    def read_json(self):\n",
        "        \"\"\"\n",
        "        Read the JSON file and return the data.\n",
        "\n",
        "        Returns:\n",
        "        data (dict): The data read from the JSON file.\n",
        "        \"\"\"\n",
        "        with open(self.json_file, ’r’) as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        \"\"\"\n",
        "        Check if the URL is valid.\n",
        "\n",
        "        Parameters:\n",
        "        url (str): The URL to be checked.\n",
        "\n",
        "        Returns:\n",
        "        bool: True if the URL is valid, False otherwise.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                if response.status == 200 and ’image’ in response.info().get_content_type():\n",
        "                    return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def download_image(self, url, category, term, pbar):\n",
        "        \"\"\"\n",
        "        Download the image from the given URL.\n",
        "\n",
        "        Parameters:\n",
        "        url (str): The URL of the image to be downloaded.\n",
        "        category (str): The category of the image.\n",
        "        term (str): The term or keyword associated with the image.\n",
        "        pbar (tqdm): The progress bar object.\n",
        "\n",
        "        Returns:\n",
        "        str: A message indicating the status of the download.\n",
        "        \"\"\"\n",
        "        if not self.is_valid_url(url):\n",
        "            pbar.update(1)\n",
        "            return f\"Invalid URL: {url}\"\n",
        "\n",
        "        category_dir = os.path.join(self.download_dir, category)\n",
        "        if not os.path.exists(category_dir):\n",
        "            os.makedirs(category_dir)\n",
        "\n",
        "        term_dir = os.path.join(category_dir, term)\n",
        "        if not os.path.exists(term_dir):\n",
        "            os.makedirs(term_dir)\n",
        "\n",
        "        filename = os.path.join(term_dir, os.path.basename(urlparse(url).path))\n",
        "\n",
        "        self.filename.add(filename) # Record the filename directory\n",
        "\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, filename)\n",
        "            pbar.update(1)\n",
        "            return f\"Downloaded: {url}\"\n",
        "        except Exception as e:\n",
        "            pbar.update(1)\n",
        "            return f\"Failed to download {url}: {str(e)}\"\n",
        "\n",
        "    def download_images(self):\n",
        "        \"\"\"\n",
        "        Download images from the JSON file.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        data = self.read_json()\n",
        "        download_tasks = []\n",
        "\n",
        "        total_images = sum(len(urls) for terms in data.values() for urls in terms.values())\n",
        "        with tqdm(total=total_images, desc=\"Downloading images\") as pbar:\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "                for category, terms in data.items():\n",
        "                    for term, urls in terms.items():\n",
        "                        for url in urls:\n",
        "                            download_tasks.append(executor.submit(self.download_image, url,\n",
        "                            category, term, pbar))\n",
        "                            time.sleep(self.delay) # Polite delay\n",
        "\n",
        "                for future in concurrent.futures.as_completed(download_tasks):\n",
        "                    print(future.result())\n",
        "\n",
        "        self.export_filename()\n",
        "\n",
        "    def export_filename(self):\n",
        "    \"\"\"\n",
        "    Export the filename directories to a text file.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    with open(\"filename.txt\", \"w\") as file:\n",
        "        for filename in sorted(self.filename):\n",
        "            file.write(f\"{filename}\\n\")"
      ],
      "metadata": {
        "id": "6qxUBXEC-kJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downloader = ImageDownloader(json_file=\"image_urls.json\", download_dir=\"Dataset\", max_workers=4, delay=1)\n",
        "downloader.download_images()\n",
        "downloader.export_filename()"
      ],
      "metadata": {
        "id": "QpB3K81jAMes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Process the data"
      ],
      "metadata": {
        "id": "OVIyhszEAtMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "id": "bYWwDQ1-At8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_preprocess_images(image_dir):\n",
        "    \"\"\"\n",
        "    Check and preprocess images in the specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    image_dir (str): The directory containing the images to be checked and preprocessed.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(image_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    # Check if image is smaller than 50x50 pixels\n",
        "                    if img.size[0] < 50 or img.size[1] < 50:\n",
        "                        os.remove(file_path)\n",
        "                        print(f\"Deleted {file_path}: Image too small ({img.size[0]}x{img.size[1]})\")\n",
        "                        continue\n",
        "\n",
        "                    # Convert non-RGB images to RGB\n",
        "                    if img.mode != \"RGB\":\n",
        "                        img = img.convert(\"RGB\")\n",
        "                        img.save(file_path)\n",
        "                        print(f\"Converted {file_path} to RGB\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # If file is not an image, delete it\n",
        "                os.remove(file_path)\n",
        "                print(f\"Deleted {file_path}: Not an image or corrupted file ({str(e)})\")\n",
        "\n",
        "check_and_preprocess_images(\"Dataset\")"
      ],
      "metadata": {
        "id": "GKiviMokAzLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/Clean_Dataset.zip Dataset"
      ],
      "metadata": {
        "id": "Nf3M2tp9A2Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create paths for folders\n",
        "!gdown --id 1--6fe48D9ydnTpLV1GKKqJ0pqpOXB3z_\n",
        "!unzip Clean_Dataset"
      ],
      "metadata": {
        "id": "nlAJgML4A88G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define the source and target directories\n",
        "source_dir = \"Dataset\"\n",
        "train_dir = \"data/train\"\n",
        "test_dir = \"data/test\"\n",
        "\n",
        "# Create the target directories if they don’t exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Initialize a dictionary to hold file paths for each class\n",
        "class_files = defaultdict(list)\n",
        "\n",
        "# Read the file paths from the text file\n",
        "with open(\"filename.txt\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            # Extract the class name from the path\n",
        "            parts = line.split(\"/\")\n",
        "            class_name = parts[2] # Structure Dataset/category/class/image.jpg\n",
        "            class_files[class_name].append(line)\n",
        "\n",
        "# Move images to the train and test directories\n",
        "for class_name, files in class_files.items():\n",
        "    # Create the train and test directories for the class\n",
        "    train_class_dir = os.path.join(train_dir, class_name)\n",
        "    test_class_dir = os.path.join(test_dir, class_name)\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "    # Move 19 images to train and 1 image to test\n",
        "    for i, file_path in enumerate(files):\n",
        "        if i == 0:\n",
        "            shutil.copy(file_path, test_class_dir)\n",
        "        elif i < 20:\n",
        "            shutil.copy(file_path, train_class_dir)\n",
        "\n",
        "print(\"Dataset organization complete!\")"
      ],
      "metadata": {
        "id": "YVVWPcIhBF_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/data.zip data"
      ],
      "metadata": {
        "id": "4du3c5CrBIAj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}